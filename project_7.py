# -*- coding: utf-8 -*-
"""Project 7.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1VAbB_NXau8UOC34iF0c9L8BuWNMSCBkT
"""

from google.colab import drive
drive.mount('/content/drive')

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
sns.set()
from matplotlib import pyplot
from scipy import stats
from sklearn.model_selection import train_test_split
from sklearn import preprocessing
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import explained_variance_score, mean_squared_error, r2_score, mean_absolute_error
from collections import Counter
from keras import callbacks
import tensorflow as tf
from tensorflow.keras.layers import Dense, Dropout,InputLayer
from tensorflow.keras.models import Sequential
from keras.layers import BatchNormalization
from tensorflow.keras import regularizers
from tensorflow.keras import backend
from random import shuffle
from keras.callbacks import ModelCheckpoint
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.optimizers import RMSprop
import warnings
warnings.filterwarnings("ignore")

"""Reading Dataset and Feature Elimination
- Read the dataset properly - Print the overview of the data (statistical summary, shape, info, etc) - Eliminate the unique features from the dataset with proper reasoning
"""

#Read in the data from excel (I converted form csv)
data = pd.read_excel('/content/drive/MyDrive/Churn.xlsx')

data.head() #see a small sample of the data

#Information about the data
data.info()

data.shape#what is the shape of the data

#finding the null values
data.isnull().sum()

"""No NA so no need to drop NA, check duplicates"""

duplicate = data[data.duplicated()]
duplicate

"""Also no duplicates so no need to drop those either"""

#Checking the dtypes of the variables in the data
data.dtypes

"""No date time or other suspisious dtypes"""

data.drop(['RowNumber','CustomerId','Surname'],axis=1,inplace=True)
#Row number is not real data so I don't need it. It is noise. Surname and CustomerId I am dropping for a few reasons 1) it is not at all related to anything 2) customers may not like having it used

"""Perform an Exploratory Data Analysis on the data
- Checked whether the dataset is balanced or not - Bivariate analysis - Use appropriate visualizations to identify the patterns and insights - Any other exploratory deep dive

Univariable
"""

# function to create labeled barplots


def labeled_barplot(data, feature, perc=False, n=None):
    """
    Barplot with percentage at the top

    data: dataframe
    feature: dataframe column
    perc: whether to display percentages instead of count (default is False)
    n: displays the top n category levels (default is None, i.e., display all levels)
    """

    total = len(data[feature])  # length of the column
    count = data[feature].nunique()
    if n is None:
        plt.figure(figsize=(count + 1, 5))
    else:
        plt.figure(figsize=(n + 1, 5))

    plt.xticks(rotation=90, fontsize=15)
    ax = sns.countplot(
        data=data,
        x=feature,
        palette="Paired",
        order=data[feature].value_counts().index[:n].sort_values(),
    )

    for p in ax.patches:
        if perc == True:
            label = "{:.1f}%".format(
                100 * p.get_height() / total
            )  # percentage of each class of the category
        else:
            label = p.get_height()  # count of each level of the category

        x = p.get_x() + p.get_width() / 2  # width of the plot
        y = p.get_height()  # height of the plot

        ax.annotate(
            label,
            (x, y),
            ha="center",
            va="center",
            size=12,
            xytext=(0, 5),
            textcoords="offset points",
        )  # annotate the percentage

    plt.show()  # show the plot

# function to plot a boxplot and a histogram along the same scale.

#lets look at a boxplot and histogram to see if each vairable is "normal"
def histogram_boxplot(data, feature, figsize=(12, 7), kde=False, bins=None):
    """
    Boxplot and histogram combined

    data: dataframe
    feature: dataframe column
    figsize: size of figure (default (12,7))
    kde: whether to the show density curve (default False)
    bins: number of bins for histogram (default None)
    """
    f2, (ax_box2, ax_hist2) = plt.subplots(
        nrows=2,  # Number of rows of the subplot grid= 2
        sharex=True,  # x-axis will be shared among all subplots
        gridspec_kw={"height_ratios": (0.25, 0.75)},
        figsize=figsize,
    )  # creating the 2 subplots
    sns.boxplot(
        data=data, x=feature, ax=ax_box2, showmeans=True, color="violet"
    )  # boxplot will be created and a star will indicate the mean value of the column
    sns.histplot(
        data=data, x=feature, kde=kde, ax=ax_hist2, bins=bins, palette="winter"
    ) if bins else sns.histplot(
        data=data, x=feature, kde=kde, ax=ax_hist2
    )  # For histogram
    # Add mean to the histogram
    ax_hist2.axvline(
        data[feature].median(), color="black", linestyle="-"
    )  # Add median to the histogram

data.head() # I want to be easily able to see this while working

histogram_boxplot(data, 'CreditScore')#plot credit score

"""Pretty regular most people around 650 with some outliers below 400. Mean and median are very close so it is pretty balanced"""

histogram_boxplot(data, 'Age')#plot credit score

"""Mean and median a little far off, median being about 36 and mean 38. There are many outliers above 60"""

histogram_boxplot(data, 'Balance')#plot credit score

"""Balance not balances (ironic) with the mean being 7,5000 and the median 10,000 (roughly) their are a lot of people at 0 (35% it seems)"""

histogram_boxplot(data, 'EstimatedSalary')#plot credit score

"""A different kind of distibution but is balanced. we shouls see a nice normal dist or even one consentrated lower, perhaps we appeal to high income people?"""

labeled_barplot(data, "Geography", perc=True)

"""About 1/2 in France. Other's 50% between Germany and Spain."""

labeled_barplot(data, "Gender", perc=True)

"""Slightly more males than females"""

labeled_barplot(data, "Tenure", perc=True)

"""4% new and 5% from the begining otherwse we see about the same 10% in each"""

labeled_barplot(data, "NumOfProducts", perc=True)

"""half only have one, 45% two. Almost no one has more than that"""

labeled_barplot(data, "HasCrCard", perc=True)

"""Majoroty (almost vast) have our cr card"""

labeled_barplot(data, "IsActiveMember", perc=True)

"""Slightly more active than not"""

labeled_barplot(data, "Exited", perc=True)

"""Vast majority still with us but we have lost a little over 1/5

Bivariate
"""

plt.figure(figsize=(10,7))
sns.heatmap(data.corr(),annot=True,vmin=-1,vmax=1,fmt='.2g',cmap="Spectral")# create a heatmap
plt.show()
#the heatmap will show us if any values are strongly correlated

"""No really strong correlations. Num of products and balance have a slightly negative relation and age and exited are slightly positve. This may mean that having more than one product depletes an account and that old people are closing their accounts.

Exit is our vairable of interets so plot vs that
"""

plt.figure(figsize=(15,15))#ex
sns.catplot(x='Exited',y='CreditScore',data=data.sort_values('Exited',ascending=False),kind='boxen')

"""Average score is the same, but those with very low scores seem more likely to exit"""

plt.figure(figsize=(15,15))#ex
sns.catplot(x='Exited',y='Age',data=data.sort_values('Exited',ascending=False),kind='boxen')

"""Thos who are older are more likely to exit"""

plt.figure(figsize=(15,15))#ex
sns.catplot(x='Exited',y='Tenure',data=data.sort_values('Exited',ascending=False),kind='boxen')

"""No real different, but the IQR for 0 is smaller so perhaps those who are in the middle may be more likely to exit"""

plt.figure(figsize=(15,15))#ex
sns.catplot(x='Exited',y='Balance',data=data.sort_values('Exited',ascending=False),kind='boxen')

"""Higher balance is more likely to leave"""

plt.figure(figsize=(15,15))#ex
sns.catplot(x='Exited',y='EstimatedSalary',data=data.sort_values('Exited',ascending=False),kind='boxen')

"""No noticable difference"""

def stacked_barplot(data, predictor, target):#better for comparing cateogries
    """
    Print the category counts and plot a stacked bar chart

    data: dataframe
    predictor: independent variable
    target: target variable
    """
    count = data[predictor].nunique()
    sorter = data[target].value_counts().index[-1]
    tab1 = pd.crosstab(data[predictor], data[target], margins=True).sort_values(
        by=sorter, ascending=False
    )
    print(tab1)
    print("-" * 120)
    tab = pd.crosstab(data[predictor], data[target], normalize="index").sort_values(
        by=sorter, ascending=False
    )
    tab.plot(kind="bar", stacked=True, figsize=(count + 5, 6))
    plt.legend(
        loc="lower left", frameon=False,
    )
    plt.legend(loc="upper left", bbox_to_anchor=(1, 1))
    plt.show()

stacked_barplot(data, "NumOfProducts","Exited")

"""No one with 4 stayed, only 17% if thise with 3 stayed. 35% of those with 1 left and only 8% of those with 2 left."""

stacked_barplot(data, "HasCrCard","Exited")

"""No difference here"""

stacked_barplot(data, "IsActiveMember","Exited")

"""27% of inactive left and only 14% of active left"""

stacked_barplot(data, "Gender","Exited")

"""25% or women leave compared to 17% of men"""

stacked_barplot(data, "Geography","Exited")

"""32% of Germans leave compared to 16% of French people and 17% of those in Spain. Gemrans twics as likely to leave

Illustrate the insights based on EDA
-Key meaningful observations from Bivariate analysis

1) most of our customers are still with us so we are doing something right
2)Mst have a credit card and msot have 1-2 products
3) France has over hald our customers, market to them
4) 35% or so have almost no or no balance, making us litlte if any money
5) income and years with us are pretty consitant
6) we have some older people but most seem 25-40 years old
7)creidt socres are pretty average with a fair number of perfect/almost perfect scores

People are more likely to exit if they have low credit scores, are old (above 40), have a higher account balance,  have 3-4 products, are inactive, are women, or are German

I suggest we market to those with higher credit scores, encourage people to have exactly two products, determine if women are leaving due to account mergers (marriage) or for other reasons, figure out if Germans are leaving due to language problems or other reasons, and figure out why people with higher balances are leaving.

Data Pre-processing
- Split the target variable and predictors - Split the data into train and test - Rescale the data
"""

## Storing required categorical variables in cat_dat to apply dummification
cat_data = ['Geography','Gender']
data = pd.get_dummies(data,columns=cat_data,drop_first= True)

#again, ke is exit as we want to see how we can prevent this from occuring
##Splitting Independent and dependent variable in X and Y respectively
X = data.drop(['Exited'],axis=1)
Y = data[['Exited']]
print(X.shape)
print(Y.shape)

# Splitting the dataset into the Training and Testing set.

X_train, X_test, y_train, y_test = train_test_split(X,Y, test_size = 0.3, random_state = 42)

X_train.shape #look at what the training data looks like

X_train.head() #see a sample of the train, make sure exited isn't there

# Checking the shape
print(X_train.shape)
print(X_test.shape)
print(y_train.shape)
print(y_test.shape)

## Scaling the data
sc=StandardScaler()
scaler = StandardScaler().fit(X_train)
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)
X_test

"""Model building
- Build Neural Network
"""

import tensorflow.keras as keras
# create a NN with 3 hiden layer
model = keras.Sequential([
keras.layers.Dense(11, input_shape=(11,),kernel_initializer='he_normal', activation='relu'),
keras.layers.BatchNormalization(),
keras.layers.Dense(9, activation='relu'),
keras.layers.BatchNormalization(),
keras.layers.Dense(4, activation='relu'),
keras.layers.BatchNormalization(),
keras.layers.Dense(1,activation='sigmoid')
])

model.summary()

"""Model Performance Improvement
-Comment on which scaler is right for model performance evaluation and why? - Find the optimal threshold using ROC-AUC curves - Comment on model performance - Can model performance be improved? check and comment - Build another model to implement these improvements - Include all the model which were trained to reach at the final one
"""

def make_confusion_matrix(cf,
                          group_names=None,
                          categories='auto',
                          count=True,
                          percent=True,
                          cbar=True,
                          xyticks=True,
                          xyplotlabels=True,
                          sum_stats=True,
                          figsize=None,
                          cmap='Blues',
                          title=None):
    '''
    This function will make a pretty plot of an sklearn Confusion Matrix cm using a Seaborn heatmap visualization.
    Arguments
    '''


    # CODE TO GENERATE TEXT INSIDE EACH SQUARE
    blanks = ['' for i in range(cf.size)]

    if group_names and len(group_names)==cf.size:
        group_labels = ["{}\n".format(value) for value in group_names]
    else:
        group_labels = blanks

    if count:
        group_counts = ["{0:0.0f}\n".format(value) for value in cf.flatten()]
    else:
        group_counts = blanks

    if percent:
        group_percentages = ["{0:.2%}".format(value) for value in cf.flatten()/np.sum(cf)]
    else:
        group_percentages = blanks

    box_labels = [f"{v1}{v2}{v3}".strip() for v1, v2, v3 in zip(group_labels,group_counts,group_percentages)]
    box_labels = np.asarray(box_labels).reshape(cf.shape[0],cf.shape[1])


    # CODE TO GENERATE SUMMARY STATISTICS & TEXT FOR SUMMARY STATS
    if sum_stats:
        #Accuracy is sum of diagonal divided by total observations
        accuracy  = np.trace(cf) / float(np.sum(cf))

        #if it is a binary confusion matrix, show some more stats
        if len(cf)==2:
            #Metrics for Binary Confusion Matrices
            precision = cf[1,1] / sum(cf[:,1])
            recall    = cf[1,1] / sum(cf[1,:])
            f1_score  = 2*precision*recall / (precision + recall)
            stats_text = "\n\nAccuracy={:0.3f}\nPrecision={:0.3f}\nRecall={:0.3f}\nF1 Score={:0.3f}".format(
                accuracy,precision,recall,f1_score)
        else:
            stats_text = "\n\nAccuracy={:0.3f}".format(accuracy)
    else:
        stats_text = ""


    # SET FIGURE PARAMETERS ACCORDING TO OTHER ARGUMENTS
    if figsize==None:
        #Get default figure size if not set
        figsize = plt.rcParams.get('figure.figsize')

    if xyticks==False:
        #Do not show categories if xyticks is False
        categories=False


    # MAKE THE HEATMAP VISUALIZATION
    plt.figure(figsize=figsize)
    sns.heatmap(cf,annot=box_labels,fmt="",cmap=cmap,cbar=cbar,xticklabels=categories,yticklabels=categories)

    if xyplotlabels:
        plt.ylabel('True label')
        plt.xlabel('Predicted label' + stats_text)
    else:
        plt.xlabel(stats_text)

    if title:
        plt.title(title)

from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score, precision_recall_curve, auc
#we want to see hoe we can minimize people leaving, thus we don't want a lot of flase negative as we want to see what cause them to leave, not stay
#We want to mimize FP and since this is binary, recall will do will.

optimizer = Adam(0.001)
model.compile(optimizer=optimizer,loss='binary_crossentropy',metrics=['Recall'])
#es_cb = callbacks.EarlyStopping(monitor='val_loss', min_delta=0.001, patience=5)
history = model.fit(X_train, y_train, validation_data=(X_test, y_test),epochs=50)

pyplot.title('Loss / Mean Squared Error')
pyplot.plot(history.history['loss'], label='train')
pyplot.plot(history.history['val_loss'], label='validate')
pyplot.legend()
pyplot.show()
#look at the loss over each epoch regardng MSE

"""Our loss is stable around 0.35, lower for val, this is a good sign. Maybe a little high"""

## Confusion Matrix on unsee test set
import seaborn as sns
y_pred1 = model.predict(X_test) #predict y based on the mode;
for i in range(len(y_test)):
    if y_pred1[i]>0.5:
        y_pred1[i]=1
    else:
        y_pred1[i]=0



cm=confusion_matrix(y_test, y_pred1)
labels = ['True Negative','False Positive','False Negative','True Positive']
#categories = [ 'Not_Fraud','Fraud']
make_confusion_matrix(cm,
                      group_names=labels,
                      #categories=categories,
                      cmap='Blues')

"""our recall is terrible, lets fix that"""

from sklearn.metrics import roc_curve




# predict probabilities
yhat1 = model.predict(X_test)
# keep probabilities for the positive outcome only
yhat1 = yhat1[:, 0]
# calculate roc curves
fpr, tpr, thresholds1 = roc_curve(y_test, yhat1)
# calculate the g-mean for each threshold
gmeans1 = np.sqrt(tpr * (1-fpr))
# locate the index of the largest g-mean
ix = np.argmax(gmeans1)
print('Best Threshold=%f, G-Mean=%.3f' % (thresholds1[ix], gmeans1[ix]))
# plot the roc curve for the model
pyplot.plot([0,1], [0,1], linestyle='--', label='No Skill')
pyplot.plot(fpr, tpr, marker='.')
pyplot.scatter(fpr[ix], tpr[ix], marker='o', color='black', label='Best')
# axis labels
pyplot.xlabel('False Positive Rate')
pyplot.ylabel('True Positive Rate')
pyplot.legend()
# show the plot
pyplot.show()

"""We have the best index, where the distance to the tope right is least, lets use it to find"""

y_pred_e1=model.predict(X_test)
y_pred_e1 = (y_pred_e1 > thresholds1[ix])
y_pred_e1

cm=confusion_matrix(y_test, y_pred_e1)
labels = ['True Negative','False Positive','False Negative','True Positive']
#categories = [ 'Not_Fraud','Fraud']
make_confusion_matrix(cm,
                      group_names=labels,
                      #categories=categories,
                      cmap='Blues')

"""Much better, but let's keept going. We need to get false positives as close to 0 as possible

I want to to a few thing: add new layer, expand current layer, and add in drop out to see if any work

I will then use rnadom search cv on the best of our models to see if it can get even better
"""

#add the extra layer
model2 = keras.Sequential([
keras.layers.Dense(11, input_shape=(11,),kernel_initializer='he_normal', activation='relu'),
keras.layers.BatchNormalization(),
keras.layers.Dense(9, activation='relu'),
keras.layers.BatchNormalization(),
keras.layers.Dense(6, activation='relu'),
keras.layers.BatchNormalization(),
keras.layers.Dense(4, activation='relu'),
keras.layers.BatchNormalization(),
keras.layers.Dense(1,activation='sigmoid')
])

optimizer = Adam(0.001)
model2.compile(optimizer=optimizer,loss='binary_crossentropy',metrics=['Recall'])
#es_cb = callbacks.EarlyStopping(monitor='val_loss', min_delta=0.001, patience=5)
history2 = model2.fit(X_train, y_train, validation_data=(X_test, y_test),epochs=50)

pyplot.title('Loss / Mean Squared Error')
pyplot.plot(history2.history['loss'], label='train')
pyplot.plot(history2.history['val_loss'], label='validate')
pyplot.legend()
pyplot.show()
#look at the loss over each epoch regardng MSE

"""Very similat to the original, no improvment that is noteworthy"""

# predict probabilities
yhat2 = model2.predict(X_test)
# keep probabilities for the positive outcome only
yhat2 = yhat2[:, 0]
# calculate roc curves
fpr, tpr, thresholds2 = roc_curve(y_test, yhat2)
# calculate the g-mean for each threshold
gmeans2 = np.sqrt(tpr * (1-fpr))
# locate the index of the largest g-mean
ix = np.argmax(gmeans2)
print('Best Threshold=%f, G-Mean=%.3f' % (thresholds2[ix], gmeans2[ix]))
# plot the roc curve for the model
pyplot.plot([0,1], [0,1], linestyle='--', label='No Skill')
pyplot.plot(fpr, tpr, marker='.')
pyplot.scatter(fpr[ix], tpr[ix], marker='o', color='black', label='Best')
# axis labels
pyplot.xlabel('False Positive Rate')
pyplot.ylabel('True Positive Rate')
pyplot.legend()
# show the plot
pyplot.show()

y_pred_e2=model2.predict(X_test)
y_pred_e2 = (y_pred_e2 > thresholds2[ix])#predict the y based on new model
y_pred_e2

cm2=confusion_matrix(y_test, y_pred_e2)
labels = ['True Negative','False Positive','False Negative','True Positive']
#categories = [ 'Not_Fraud','Fraud']
make_confusion_matrix(cm2,
                      group_names=labels,
                      #categories=categories,
                      cmap='Blues')

"""A little worse, remove extra layer"""

#create larger layer (2x)
model3 = keras.Sequential([
keras.layers.Dense(22, input_shape=(11,),kernel_initializer='he_normal', activation='relu'),
keras.layers.BatchNormalization(),
keras.layers.Dense(18, activation='relu'),
keras.layers.BatchNormalization(),
keras.layers.Dense(12, activation='relu'),
keras.layers.BatchNormalization(),
keras.layers.Dense(1,activation='sigmoid')
])

optimizer = Adam(0.001)
model3.compile(optimizer=optimizer,loss='binary_crossentropy',metrics=['Precision'])
#es_cb = callbacks.EarlyStopping(monitor='val_loss', min_delta=0.001, patience=5)
history3 = model3.fit(X_train, y_train, validation_data=(X_test, y_test),epochs=50)

pyplot.title('Loss / Mean Squared Error')
pyplot.plot(history3.history['loss'], label='train')
pyplot.plot(history3.history['val_loss'], label='validate')
pyplot.legend()
pyplot.show()
#look at the loss over each epoch regardng MSE

"""Ttan loss a little lower than val and val seems to be icnreasing."""

# predict probabilities
yhat3 = model3.predict(X_test)
# keep probabilities for the positive outcome only
yhat3 = yhat3[:, 0]
# calculate roc curves
fpr, tpr, thresholds3 = roc_curve(y_test, yhat3)
# calculate the g-mean for each threshold
gmeans3 = np.sqrt(tpr * (1-fpr))
# locate the index of the largest g-mean
ix = np.argmax(gmeans3)
print('Best Threshold=%f, G-Mean=%.3f' % (thresholds3[ix], gmeans3[ix]))
# plot the roc curve for the model
pyplot.plot([0,1], [0,1], linestyle='--', label='No Skill')
pyplot.plot(fpr, tpr, marker='.')
pyplot.scatter(fpr[ix], tpr[ix], marker='o', color='black', label='Best')
# axis labels
pyplot.xlabel('False Positive Rate')
pyplot.ylabel('True Positive Rate')
pyplot.legend()
# show the plot
pyplot.show()

y_pred_e3=model3.predict(X_test)
y_pred_e3 = (y_pred_e3 > thresholds3[ix])#predict y based on model
y_pred_e3
#let's see if our trehsold is lower or higher
thresholds3[ix]

cm3=confusion_matrix(y_test, y_pred_e3)
labels = ['True Negative','False Positive','False Negative','True Positive']
#categories = [ 'Not_Fraud','Fraud']
make_confusion_matrix(cm3,
                      group_names=labels,
                      #categories=categories,
                      cmap='Blues')

"""Getting better, keep layers larger"""

#create te model with batch and dropout
model4 = keras.Sequential([
keras.layers.Dense(22, input_shape=(11,),kernel_initializer='he_normal', activation='relu'),
keras.layers.BatchNormalization(),
keras.layers.Dropout(0.5),
keras.layers.Dense(18, activation='relu'),
keras.layers.BatchNormalization(),
keras.layers.Dropout(0.5),
keras.layers.Dense(12, activation='relu'),
keras.layers.BatchNormalization(),
keras.layers.Dropout(0.5),
keras.layers.Dropout(0.5),
keras.layers.Dense(1,activation='sigmoid')
])

optimizer = Adam(0.001)
model4.compile(optimizer=optimizer,loss='binary_crossentropy',metrics=['Recall'])
#es_cb = callbacks.EarlyStopping(monitor='val_loss', min_delta=0.001, patience=5)
history4 = model4.fit(X_train, y_train, validation_data=(X_test, y_test),epochs=50)

pyplot.title('Loss / Mean Squared Error')
pyplot.plot(history4.history['loss'], label='train')
pyplot.plot(history4.history['val_loss'], label='validate')
pyplot.legend()
pyplot.show()
#look at the loss over each epoch regardng MSE

"""Loss is a little higher but is decrasing, after more epochs it MAY be lower than others"""

# predict probabilities
yhat4 = model4.predict(X_test)
# keep probabilities for the positive outcome only
yhat4 = yhat4[:, 0]
# calculate roc curves
fpr, tpr, thresholds4 = roc_curve(y_test, yhat4)
# calculate the g-mean for each threshold
gmeans4 = np.sqrt(tpr * (1-fpr))
# locate the index of the largest g-mean
ix = np.argmax(gmeans4)
print('Best Threshold=%f, G-Mean=%.3f' % (thresholds4[ix], gmeans4[ix]))
# plot the roc curve for the model
pyplot.plot([0,1], [0,1], linestyle='--', label='No Skill')
pyplot.plot(fpr, tpr, marker='.')
pyplot.scatter(fpr[ix], tpr[ix], marker='o', color='black', label='Best')
# axis labels
pyplot.xlabel('False Positive Rate')
pyplot.ylabel('True Positive Rate')
pyplot.legend()
# show the plot
pyplot.show()

y_pred_e4=model4.predict(X_test)
y_pred_e4 = (y_pred_e4 > thresholds4[ix])#predict y based on the model
y_pred_e4
#let's see if our trehsold is lower or higher
thresholds4[ix]

cm4=confusion_matrix(y_test, y_pred_e4)
labels = ['True Negative','False Positive','False Negative','True Positive']
#categories = [ 'Not_Fraud','Fraud']
make_confusion_matrix(cm4,
                      group_names=labels,
                      #categories=categories,
                      cmap='Blues')

"""Recall is a little wors,e don't use dropout
Grib search CV
"""

backend.clear_session()#start CV search
np.random.seed(42)
import random
random.seed(42)
tf.random.set_seed(42)

def create_model_v4(lr,batch_size):
    np.random.seed(1337)
    model =  keras.Sequential([
    keras.layers.Dense(22, input_shape=(11,),kernel_initializer='he_normal', activation='relu'),
    keras.layers.BatchNormalization(),
    keras.layers.Dense(18, activation='relu'),
    keras.layers.BatchNormalization(),
    keras.layers.Dense(12, activation='relu'),
    keras.layers.BatchNormalization(),
    keras.layers.Dense(1,activation='sigmoid')
    ])
    #compile model
    optimizer = tf.keras.optimizers.Adam(learning_rate=lr)
    model.compile(optimizer = optimizer,loss = 'binary_crossentropy', metrics = ['Precision'])
    return model

"""Use Grid Search CV to optimize learning rate and batch size"""

from tensorflow.keras.wrappers.scikit_learn import KerasClassifier
from sklearn.model_selection import RandomizedSearchCV
keras_estimator = KerasClassifier(build_fn=create_model_v4, verbose=1)
# define the grid search parameters
param_random = {
    'batch_size':[32, 64, 128],
    "lr":[0.01,0.1,0.001],}

kfold_splits = 3
random= RandomizedSearchCV(estimator=keras_estimator,
                    verbose=1,
                    cv=kfold_splits,
                    param_distributions=param_random,n_jobs=-1)

random_result = random.fit(X_train, y_train,validation_split=0.2,verbose=1)

# Summarize results
print("Best: %f using %s" % (random_result.best_score_, random_result.best_params_))
means = random_result.cv_results_['mean_test_score']
stds = random_result.cv_results_['std_test_score']
params = random_result.cv_results_['params']

estimator_v4=create_model_v4(batch_size=random_result.best_params_['batch_size'],lr=random_result.best_params_['lr'])

estimator_v4.summary()
#now that we have best paramaters we should now create the final model using them

optimizer = tf.keras.optimizers.Adam(random_result.best_params_['lr'])
estimator_v4.compile(loss='binary_crossentropy',optimizer=optimizer,metrics=['Recall'])
history5=estimator_v4.fit(X_train, y_train, epochs=50, batch_size = 64,verbose=1,validation_split=0.2)

pyplot.title('Loss / Mean Squared Error')
pyplot.plot(history5.history['loss'], label='train')
pyplot.plot(history5.history['val_loss'], label='validate')
pyplot.legend()
pyplot.show()
#look at the loss over each epoch regardng MSE

"""Besy tain loss but val loss is higher. bith very unstable and thus may not be reliable"""

pyplot.title('Loss / Mean Squared Error')
pyplot.plot(history5.history['loss'], label='train')
pyplot.plot(history5.history['val_loss'], label='validate')
pyplot.legend()
pyplot.show()
#look at the loss over each epoch regardng MSE

# predict probabilities
yhat5 = estimator_v4.predict(X_test)
# keep probabilities for the positive outcome only
yhat5 = yhat5[:, 0]
# calculate roc curves
fpr, tpr, thresholds5 = roc_curve(y_test, yhat5)
# calculate the g-mean for each threshold
gmeans5 = np.sqrt(tpr * (1-fpr))
# locate the index of the largest g-mean
ix = np.argmax(gmeans5)
print('Best Threshold=%f, G-Mean=%.3f' % (thresholds5[ix], gmeans5[ix]))
# plot the roc curve for the model
pyplot.plot([0,1], [0,1], linestyle='--', label='No Skill')
pyplot.plot(fpr, tpr, marker='.')
pyplot.scatter(fpr[ix], tpr[ix], marker='o', color='black', label='Best')
# axis labels
pyplot.xlabel('False Positive Rate')
pyplot.ylabel('True Positive Rate')
pyplot.legend()
# show the plot

y_pred_e5=estimator_v4.predict(X_test)#predict the new y
y_pred_e5 = (y_pred_e5 >= thresholds5[ix])
y_pred_e5

#Calculating the confusion matrix

cm5=confusion_matrix(y_test, y_pred_e5)
labels = ['True Positive','False Negative','False Positive','True Negative']
categories = [ 'Not Changing Job','Changing Job']
make_confusion_matrix(cm5,
                      group_names=labels,
                      categories=categories,
                      cmap='Blues')

"""A little worse than our best precision.

CV random search
"""

backend.clear_session()
np.random.seed(42)
import random
random.seed(42)
tf.random.set_seed(42)

"""Use random cv search to optimize learning rate and batch size"""

def create_model_v5(lr,batch_size):
    np.random.seed(1337)
    model = keras.Sequential([
    keras.layers.Dense(22, input_shape=(11,),kernel_initializer='he_normal', activation='relu'),
    keras.layers.BatchNormalization(),
    keras.layers.Dense(18, activation='relu'),
    keras.layers.BatchNormalization(),
    keras.layers.Dense(8, activation='relu'),
    keras.layers.BatchNormalization(),
    keras.layers.Dense(1,activation='sigmoid')
    ])

    #compile model
    optimizer = tf.keras.optimizers.Adam(learning_rate=lr)
    model.compile(optimizer = optimizer,loss = 'binary_crossentropy', metrics = ['Recall'])
    return model

keras_estimator = KerasClassifier(build_fn=create_model_v4, verbose=1)
# define the grid search parameters
param_random = {
    'batch_size':[32, 64, 128],
    "lr":[0.01,0.1,0.001],}

kfold_splits = 3
random= RandomizedSearchCV(estimator=keras_estimator,
                    verbose=1,
                    cv=kfold_splits,
                    param_distributions=param_random,n_jobs=-1)

random_result = random.fit(X_train, y_train,validation_split=0.2,verbose=1)

# Summarize results
print("Best: %f using %s" % (random_result.best_score_, random_result.best_params_))
means = random_result.cv_results_['mean_test_score']
stds = random_result.cv_results_['std_test_score']
params = random_result.cv_results_['params']

estimator_v5=create_model_v5(batch_size=random_result.best_params_['batch_size'],lr=random_result.best_params_['lr'])

estimator_v5.summary()

optimizer = tf.keras.optimizers.Adam(random_result.best_params_['lr'])
estimator_v5.compile(loss='binary_crossentropy',optimizer=optimizer,metrics=['Recall'])
history6=estimator_v5.fit(X_train, y_train, epochs=50, batch_size = 64, verbose=1,validation_split=0.2)

pyplot.title('Loss / Mean Squared Error')
pyplot.plot(history6.history['loss'], label='train')
pyplot.plot(history6.history['val_loss'], label='validate')
pyplot.legend()
pyplot.show()
#look at the loss over each epoch regardng MSE

"""Val is increasing fast, this may be a bad sign"""

pyplot.title('Loss / Mean Squared Error')
pyplot.plot(history6.history['loss'], label='train')
pyplot.plot(history6.history['val_loss'], label='validate')
pyplot.legend()
pyplot.show()
#look at the loss over each epoch regardng MSE

# predict probabilities
yhat6 = estimator_v5.predict(X_test)
# keep probabilities for the positive outcome only
yhat6 = yhat6[:, 0]
# calculate roc curves
fpr, tpr, thresholds6 = roc_curve(y_test, yhat6)
# calculate the g-mean for each threshold
gmeans6 = np.sqrt(tpr * (1-fpr))
# locate the index of the largest g-mean
ix = np.argmax(gmeans6)
print('Best Threshold=%f, G-Mean=%.3f' % (thresholds6[ix], gmeans6[ix]))
# plot the roc curve for the model
pyplot.plot([0,1], [0,1], linestyle='--', label='No Skill')
pyplot.plot(fpr, tpr, marker='.')
pyplot.scatter(fpr[ix], tpr[ix], marker='o', color='black', label='Best')
# axis labels
pyplot.xlabel('False Positive Rate')
pyplot.ylabel('True Positive Rate')
pyplot.legend()
# show the plot
pyplot.show()

y_pred_e6=estimator_v5.predict(X_test)
y_pred_e6 = (y_pred_e6 > thresholds6[ix])
y_pred_e6

#Calculating the confusion matrix

cm6=confusion_matrix(y_test, y_pred_e4)
labels = ['True Positive','False Negative','False Positive','True Negative']
categories = [ 'Not Changing Job','Changing Job']
make_confusion_matrix(cm6,
                      group_names=labels,
                      categories=categories,
                      cmap='Blues')

"""So far the only modle to really impove is larger layers, let's try one more thing

Use dask to see if it is any better
"""

!pip install dask-ml

# importing library
from dask_ml.model_selection import GridSearchCV as DaskGridSearchCV

def create_model_v6(lr,batch_size):
    np.random.seed(1337)
    model = keras.Sequential([
    keras.layers.Dense(11, input_shape=(11,),kernel_initializer='he_normal', activation='relu'),
    keras.layers.BatchNormalization(),
    keras.layers.Dense(9, activation='relu'),
    keras.layers.BatchNormalization(),
    keras.layers.Dense(4, activation='relu'),
    keras.layers.BatchNormalization(),
    keras.layers.Dense(1,activation='sigmoid')
    ])


    #compile model
    optimizer = tf.keras.optimizers.Adam(learning_rate=lr)
    model.compile(optimizer = optimizer,loss = 'binary_crossentropy', metrics = ['accuracy'])
    return model

keras_estimator = KerasClassifier(build_fn=create_model_v6, verbose=1)
# define the grid search parameters
param_grid = {
    'batch_size':[64,32, 128],
    "lr":[0.001,0.01,0.1],}

kfold_splits = 3
dask = DaskGridSearchCV(estimator=keras_estimator,
                    cv=kfold_splits,
                    param_grid=param_grid,n_jobs=-1)

import time

# store starting time
begin = time.time()


dask_result = dask.fit(X_train, y_train,validation_split=0.2,verbose=1)

# Summarize results
print("Best: %f using %s" % (dask_result.best_score_, dask_result.best_params_))
means = dask_result.cv_results_['mean_test_score']
stds = dask_result.cv_results_['std_test_score']
params = dask_result.cv_results_['params']

time.sleep(1)
# store end time
end = time.time()

# total time taken
print(f"Total runtime of the program is {end - begin}")

"""This took way too long but I would like to try keras tuner to see if it is better"""

!pip install keras-tuner

from tensorflow import keras
from tensorflow.keras import layers
from kerastuner.tuners import RandomSearch

backend.clear_session()
np.random.seed(42)
import random
random.seed(42)
tf.random.set_seed(42)

"""Earlier he had predefined # of hidden layer, we wil now tune that allong with learning rate and batch size to see if we can improve"""

def build_model(h):
    model = keras.Sequential()
    for i in range(h.Int('num_layers', 2, 10)):
        model.add(layers.Dense(units=h.Int('units_' + str(i),
                                            min_value=2,
                                            max_value=36,
                                            step=2),
                               activation='relu'))
    model.add(layers.Dense(1, activation='sigmoid'))
    model.compile(
        optimizer=keras.optimizers.Adam(
            h.Choice('learning_rate', [1e-2, 1e-3, 1e-4])),
        loss='binary_crossentropy',
        metrics=['Recall'])#recall needs more weight than anything else as again we care more about fidning people who will leave and not misisng any of them
    return model

# use rnadom search, tkaes longer but can be better
import keras_tuner
tuner = RandomSearch(
    build_model,
    objective=keras_tuner.Objective("val_recall", direction="max"),
    max_trials=5,
    executions_per_trial=3,
     project_name='Best_params')

tuner.search(X_train, y_train,
             epochs=5,
             validation_split = 0.2)
#now we want to use training data, which will be split vinto val too, to find bets params

## Printing the best models with their hyperparameters
tuner.results_summary()

"""Our best has  hidden  layers  a leanring rate of 0.01 I can't find anything from batch size but 32 has worked well in the past so I will use that. Use layer size from best"""

backend.clear_session()
np.random.seed(42)
import random
random.seed(42)
tf.random.set_seed(42)

#create the new model
model7 = Sequential()
model7.add(Dense(30,activation='relu',kernel_initializer='he_uniform',input_dim = X_train.shape[1]))
model7.add(BatchNormalization())
model7.add(Dense(12,activation='relu',kernel_initializer='he_uniform'))
model7.add(BatchNormalization())
model7.add(Dense(16,activation='relu',kernel_initializer='he_uniform'))
model7.add(BatchNormalization())
model7.add(Dense(24,activation='relu',kernel_initializer='he_uniform'))
model7.add(BatchNormalization())
model7.add(Dense(12,activation='relu',kernel_initializer='he_uniform'))
model7.add(BatchNormalization())
model7.add(Dense(18,activation='relu',kernel_initializer='he_uniform'))
model7.add(BatchNormalization())
model7.add(Dense(28,activation='relu',kernel_initializer='he_uniform'))
model7.add(BatchNormalization())
model7.add(Dense(22,activation='relu',kernel_initializer='he_uniform'))
model7.add(BatchNormalization())
model7.add(Dense(1, activation = 'sigmoid'))

model7.summary()

optimizer = tf.keras.optimizers.Adam(0.001)
model7.compile(loss='binary_crossentropy',optimizer=optimizer,metrics=['Recall'])

history_7 = model7.fit(X_train,y_train,batch_size=32,epochs=50,verbose=1,validation_split = 0.2)

pyplot.title('Loss / Mean Squared Error')
pyplot.plot(history_7.history['loss'], label='train')
pyplot.plot(history_7.history['val_loss'], label='validate')
pyplot.legend()
pyplot.show()
#look at the loss over each epoch regardng MSE

"""A little unsatable not not as bad as before. val may stablizie around here but it's hard to tell for sure"""

# predict probabilities
yhat7 = model7.predict(X_test)
# keep probabilities for the positive outcome only
yhat7 = yhat7[:, 0]
# calculate roc curves
fpr, tpr, thresholds7 = roc_curve(y_test, yhat7)
# calculate the g-mean for each threshold
gmeans7 = np.sqrt(tpr * (1-fpr))
# locate the index of the largest g-mean
ix = np.argmax(gmeans7)
print('Best Threshold=%f, G-Mean=%.3f' % (thresholds7[ix], gmeans7[ix]))
# plot the roc curve for the model
pyplot.plot([0,1], [0,1], linestyle='--', label='No Skill')
pyplot.plot(fpr, tpr, marker='.')
pyplot.scatter(fpr[ix], tpr[ix], marker='o', color='black', label='Best')
# axis labels
pyplot.xlabel('False Positive Rate')
pyplot.ylabel('True Positive Rate')
pyplot.legend()
# show the plot
pyplot.show()

y_pred_e7=model7.predict(X_test)
y_pred_e7 = (y_pred_e7 > thresholds7[ix])
y_pred_e7

cm7=confusion_matrix(y_test, y_pred_e7)
labels = ['True Negative','False Positive','False Negative','True Positive']
categories = [ 'Not Leaving','Leaving']
make_confusion_matrix(cm7,
                      group_names=labels,
                      categories=categories,
                      cmap='Blues')

"""After all this, it is exactly the same as cm2 for recall but our accuracy is a little better so I will consider this to be outr best model

Model Performance Evaluation
- Evaluate the model on different performance metrics and comment on the performance and scope of improvement
"""

cm=confusion_matrix(y_test, y_pred_e1)
labels = ['True Negative','False Positive','False Negative','True Positive']
#categories = [ 'Not_Fraud','Fraud']
make_confusion_matrix(cm,
                      group_names=labels,
                      #categories=categories,
                      cmap='Blues')

"""Accuracy: went slighly down
Precision: went down.
These two are due to larger number of false positive. We are identifying loyal cusitmers as attriting

Recall: went up a fair about. We have fewer flase negative, thus we are getting more of our attriting customers

F1 score: a little down due to lower precision
"""

#Accuracy as per the classification report
from sklearn import metrics
cr10=metrics.classification_report(y_test,y_pred_e7)
print(cr10)

"""As we can see, the model was much better at predicting 0s than 1s, possible due to larger number of 0s

Overall this model was not great but did well enough. May need more data to truly achieve any degree of high accuracy. The oprtimize dmodel may have a higher recall, but it's other metrics suffered, meaning it didn't really get better overall, just better at predicting who would leave and even that is minimal

Conclusion and key takeaways
- Final conclusion about the analysis

Areas of improvement
- Try removing less important features
- Try getting more customer data.
- Maybe remove outliers from some of the data
- Use larger neural networks
- Use under sampling ( i will try this at the end to see if anything improves)

Overall impression
- Our model is nothing terrible and could be used to see who may leave, but we shouldn'y rely just on it
-We miss almost 20% of those leaving

Buisness  insights

I suggest we market to those with higher credit scores, encourage people to have exactly two products, determine if women are leaving due to account mergers (marriage) or for other reasons, figure out if Germans are leaving due to language problems or other reasons, and figure out why people with higher balances are leaving

New stuff: see if we can figure out why we are missing so many people
"""

from imblearn.under_sampling import RandomUnderSampler#get random under sampler
# fit random under sampler on the train data
rus = RandomUnderSampler(random_state=1, sampling_strategy = 1)
X_train_un, y_train_un = rus.fit_resample(X_train, y_train)

# use rnadom search, tkaes longer but can be better
import keras_tuner
tuner2 = RandomSearch(
    build_model,
    objective=keras_tuner.Objective("val_recall", direction="max"),
    max_trials=5,
    executions_per_trial=3,
     project_name='Best_params2')
tuner2.search(X_train_un, y_train_un,
             epochs=5,
             validation_split = 0.2)
#now we want to use training data, which will be split vinto val too, to find bets params

## Printing the best models with their hyperparameters
tuner2.results_summary()

# we have a new model with 10 hidden layer and learnin rate of 0.0001
backend.clear_session()
np.random.seed(42)
import random
random.seed(42)
tf.random.set_seed(42)

#very different than model 7
model8 = Sequential()
model8.add(Dense(14,activation='relu',kernel_initializer='he_uniform',input_dim = X_train.shape[1]))
model8.add(BatchNormalization())
model8.add(Dense(20,activation='relu',kernel_initializer='he_uniform'))
model8.add(BatchNormalization())
model8.add(Dense(16,activation='relu',kernel_initializer='he_uniform'))
model8.add(BatchNormalization())
model8.add(Dense(6,activation='relu',kernel_initializer='he_uniform'))
model8.add(BatchNormalization())
model8.add(Dense(8,activation='relu',kernel_initializer='he_uniform'))
model8.add(BatchNormalization())
model8.add(Dense(14,activation='relu',kernel_initializer='he_uniform'))
model8.add(BatchNormalization())
model8.add(Dense(28,activation='relu',kernel_initializer='he_uniform'))
model8.add(BatchNormalization())
model8.add(Dense(2,activation='relu',kernel_initializer='he_uniform'))
model8.add(BatchNormalization())
model8.add(Dense(2,activation='relu',kernel_initializer='he_uniform'))
model8.add(BatchNormalization())
model8.add(Dense(2,activation='relu',kernel_initializer='he_uniform'))
model8.add(BatchNormalization())
model8.add(Dense(1, activation = 'sigmoid'))

optimizer = tf.keras.optimizers.Adam(0.0001)
model8.compile(loss='binary_crossentropy',optimizer=optimizer,metrics=['Recall'])

history_8 = model8.fit(X_train_un,y_train_un,batch_size=32,epochs=50,verbose=1,validation_split = 0.2)

pyplot.title('Loss / Mean Squared Error')
pyplot.plot(history_8.history['loss'], label='train')
pyplot.plot(history_8.history['val_loss'], label='validate')
pyplot.legend()
pyplot.show()
#look at the loss over each epoch regardng MSE

"""Very bad but decreasing for tain. Val is icnreasing, this model is not good at all"""

# predict probabilities
yhat8 = model8.predict(X_test)
# keep probabilities for the positive outcome only
yhat8 = yhat8[:, 0]
# calculate roc curves
fpr, tpr, thresholds8 = roc_curve(y_test, yhat8)
# calculate the g-mean for each threshold
gmeans8 = np.sqrt(tpr * (1-fpr))
# locate the index of the largest g-mean
ix = np.argmax(gmeans8)
print('Best Threshold=%f, G-Mean=%.3f' % (thresholds8[ix], gmeans8[ix]))
# plot the roc curve for the model
pyplot.plot([0,1], [0,1], linestyle='--', label='No Skill')
pyplot.plot(fpr, tpr, marker='.')
pyplot.scatter(fpr[ix], tpr[ix], marker='o', color='black', label='Best')
# axis labels
pyplot.xlabel('False Positive Rate')
pyplot.ylabel('True Positive Rate')
pyplot.legend()
# show the plot
pyplot.show()

y_pred_e8=model8.predict(X_test)
y_pred_e8 = (y_pred_e8 > thresholds8[ix])
y_pred_e8

cm8=confusion_matrix(y_test, y_pred_e8)
labels = ['True Negative','False Positive','False Negative','True Positive']
categories = [ 'Not Leaving','Leaving']
make_confusion_matrix(cm8,
                      group_names=labels,
                      categories=categories,
                      cmap='Blues')

"""Actually got way worse, barely better than guessing

FInal recommendations:
- get more data to make the model more rleiable
- Try other under and oversmapling mehtods
- offer promotions to those about to leave
- target those with hgih credit scores to open
- engoruage exactly two products
- focus on France and psain as germans leave a lot
- Offer special benefits to older people and maybe women (if research shows they aren't merging acocunts with husband but are leaving) the same cna be done to germans if we want to stay there
- Give more rewards to those with more in their acounts. This will encoruage them to stay and may even encourage others to invest more with us
"""

